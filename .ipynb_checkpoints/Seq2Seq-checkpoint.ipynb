{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 200,970 rows 18 columns\n",
      "Test: 22,331 rows 18 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050036</th>\n",
       "      <td>08/08/2016</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt is not mine</td>\n",
       "      <td>Provided no proof of debt during disputes and I have continuously disputed the debt with a credit reporting agency without any information on what the debt is from being provided by the organization.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Io, Inc.</td>\n",
       "      <td>VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>08/08/2016</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2050841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081960</th>\n",
       "      <td>12/29/2015</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Non-federal student loan</td>\n",
       "      <td>Dealing with my lender or servicer</td>\n",
       "      <td>Trouble with how payments are handled</td>\n",
       "      <td>My Private Wells Fargo loan is literally theft. I have been unable to have a balance budget with a {$250.00} monthly payment along with all my other debts, bills, and living expenses. My city 's living expenses are going up, making it difficult on a bi-weekly basis to have money to eat. Also, making difficult to save money to pursue my passion of music, as it was reason for me to pursue a college degree and required this loan. Spoke with the Loans Consolidation department and the Loan Modifi...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>WELLS FARGO &amp; COMPANY</td>\n",
       "      <td>TX</td>\n",
       "      <td>787XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>12/29/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1719881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074571</th>\n",
       "      <td>03/16/2018</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Private student loan</td>\n",
       "      <td>Dealing with your lender or servicer</td>\n",
       "      <td>Need information about your loan balance or loan terms</td>\n",
       "      <td>I admit that I owe something but the loan amount itself should not be more than {$10000.00} then whatever interest it would have accrued over time. \\n\\nAlso, the student loan company has taken my tax refund on a couple of occasions. One time it was nearly {$8000.00} The contracts they sent me if they are legitimate amount to less than {$10000.00}, nowhere near the {$30000.00} they say I owe. \\n\\nKeep in mind ACT student loans must have sold the rights to consolidate and take over my student ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Navient Solutions, LLC.</td>\n",
       "      <td>MS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>03/16/2018</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2845553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date received          Product               Sub-product  \\\n",
       "1050036    08/08/2016  Debt collection                   Medical   \n",
       "1081960    12/29/2015     Student loan  Non-federal student loan   \n",
       "1074571    03/16/2018     Student loan      Private student loan   \n",
       "\n",
       "                                         Issue  \\\n",
       "1050036  Cont'd attempts collect debt not owed   \n",
       "1081960     Dealing with my lender or servicer   \n",
       "1074571   Dealing with your lender or servicer   \n",
       "\n",
       "                                                      Sub-issue  \\\n",
       "1050036                                        Debt is not mine   \n",
       "1081960                   Trouble with how payments are handled   \n",
       "1074571  Need information about your loan balance or loan terms   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Consumer complaint narrative  \\\n",
       "1050036                                                                                                                                                                                                                                                                                                              Provided no proof of debt during disputes and I have continuously disputed the debt with a credit reporting agency without any information on what the debt is from being provided by the organization.   \n",
       "1081960  My Private Wells Fargo loan is literally theft. I have been unable to have a balance budget with a {$250.00} monthly payment along with all my other debts, bills, and living expenses. My city 's living expenses are going up, making it difficult on a bi-weekly basis to have money to eat. Also, making difficult to save money to pursue my passion of music, as it was reason for me to pursue a college degree and required this loan. Spoke with the Loans Consolidation department and the Loan Modifi...   \n",
       "1074571  I admit that I owe something but the loan amount itself should not be more than {$10000.00} then whatever interest it would have accrued over time. \\n\\nAlso, the student loan company has taken my tax refund on a couple of occasions. One time it was nearly {$8000.00} The contracts they sent me if they are legitimate amount to less than {$10000.00}, nowhere near the {$30000.00} they say I owe. \\n\\nKeep in mind ACT student loans must have sold the rights to consolidate and take over my student ...   \n",
       "\n",
       "                                  Company public response  \\\n",
       "1050036                                               NaN   \n",
       "1081960  Company chooses not to provide a public response   \n",
       "1074571                                               NaN   \n",
       "\n",
       "                         Company State ZIP code           Tags  \\\n",
       "1050036                 Io, Inc.    VA      NaN  Servicemember   \n",
       "1081960    WELLS FARGO & COMPANY    TX    787XX            NaN   \n",
       "1074571  Navient Solutions, LLC.    MS      NaN  Servicemember   \n",
       "\n",
       "        Consumer consent provided? Submitted via Date sent to company  \\\n",
       "1050036           Consent provided           Web           08/08/2016   \n",
       "1081960           Consent provided           Web           12/29/2015   \n",
       "1074571           Consent provided           Web           03/16/2018   \n",
       "\n",
       "        Company response to consumer Timely response? Consumer disputed?  \\\n",
       "1050036                       Closed              Yes                 No   \n",
       "1081960      Closed with explanation              Yes                 No   \n",
       "1074571      Closed with explanation              Yes                NaN   \n",
       "\n",
       "         Complaint ID  \n",
       "1050036       2050841  \n",
       "1081960       1719881  \n",
       "1074571       2845553  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training and test\n",
    "data = pd.read_csv('consumer_complaints.csv')\n",
    "pData = data.dropna(subset=[\"Consumer complaint narrative\", \"Issue\", \"Sub-issue\"])\n",
    "traindf, testdf = train_test_split(pData, \n",
    "                                   test_size=.10)\n",
    "# print data sizes\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRawNarrative = traindf['Consumer complaint narrative'].tolist()\n",
    "trainIssueRaw = traindf['Issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gstot\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 140 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 14 sec\n",
      "WARNING:root:Finished parsing 200,970 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 10 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=144)\n",
    "train_body_vecs = body_pp.fit_transform(trainRawNarrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 12 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 200,970 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 1 sec\n"
     ]
    }
   ],
   "source": [
    "issue_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_issue_vecs = issue_pp.fit_transform(trainIssueRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('issue_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(issue_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_issue_vecs.npy', train_issue_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (200970, 144)\n",
      "Shape of decoder input: (200970, 11)\n",
      "Shape of decoder target: (200970, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_issue_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8,002\n",
      "Size of vocabulary for issue_pp.dpkl: 115\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('issue_pp.dpkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    34500       Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 144)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 115)    34615       Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 3,555,115\n",
      "Trainable params: 3,553,315\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "#viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176853 samples, validate on 24117 samples\n",
      "Epoch 1/7\n",
      "176853/176853 [==============================] - ETA: 29:02 - loss: 5.70 - ETA: 26:03 - loss: 4.42 - ETA: 24:54 - loss: 3.65 - ETA: 24:15 - loss: 3.11 - ETA: 23:46 - loss: 2.73 - ETA: 23:26 - loss: 2.44 - ETA: 23:09 - loss: 2.21 - ETA: 22:53 - loss: 2.02 - ETA: 22:39 - loss: 1.86 - ETA: 22:24 - loss: 1.73 - ETA: 22:11 - loss: 1.62 - ETA: 21:58 - loss: 1.52 - ETA: 21:47 - loss: 1.44 - ETA: 21:36 - loss: 1.37 - ETA: 21:27 - loss: 1.30 - ETA: 21:17 - loss: 1.25 - ETA: 21:07 - loss: 1.20 - ETA: 20:56 - loss: 1.15 - ETA: 20:45 - loss: 1.11 - ETA: 20:34 - loss: 1.07 - ETA: 20:24 - loss: 1.04 - ETA: 20:14 - loss: 1.00 - ETA: 20:04 - loss: 0.98 - ETA: 19:53 - loss: 0.95 - ETA: 19:42 - loss: 0.92 - ETA: 19:32 - loss: 0.90 - ETA: 19:22 - loss: 0.88 - ETA: 19:17 - loss: 0.86 - ETA: 19:07 - loss: 0.84 - ETA: 18:57 - loss: 0.82 - ETA: 18:46 - loss: 0.81 - ETA: 18:36 - loss: 0.79 - ETA: 18:26 - loss: 0.78 - ETA: 18:16 - loss: 0.76 - ETA: 18:05 - loss: 0.75 - ETA: 17:55 - loss: 0.74 - ETA: 17:45 - loss: 0.72 - ETA: 17:35 - loss: 0.71 - ETA: 17:25 - loss: 0.70 - ETA: 17:15 - loss: 0.69 - ETA: 17:05 - loss: 0.68 - ETA: 16:56 - loss: 0.67 - ETA: 16:46 - loss: 0.66 - ETA: 16:36 - loss: 0.65 - ETA: 16:26 - loss: 0.64 - ETA: 16:16 - loss: 0.63 - ETA: 16:06 - loss: 0.62 - ETA: 15:56 - loss: 0.61 - ETA: 15:47 - loss: 0.61 - ETA: 15:37 - loss: 0.60 - ETA: 15:27 - loss: 0.59 - ETA: 15:17 - loss: 0.59 - ETA: 15:07 - loss: 0.58 - ETA: 14:58 - loss: 0.57 - ETA: 14:48 - loss: 0.56 - ETA: 14:38 - loss: 0.56 - ETA: 14:28 - loss: 0.55 - ETA: 14:18 - loss: 0.55 - ETA: 14:09 - loss: 0.54 - ETA: 13:59 - loss: 0.53 - ETA: 13:51 - loss: 0.53 - ETA: 13:42 - loss: 0.52 - ETA: 13:33 - loss: 0.52 - ETA: 13:23 - loss: 0.51 - ETA: 13:13 - loss: 0.51 - ETA: 13:04 - loss: 0.50 - ETA: 12:54 - loss: 0.50 - ETA: 12:44 - loss: 0.49 - ETA: 12:35 - loss: 0.49 - ETA: 12:25 - loss: 0.49 - ETA: 12:15 - loss: 0.48 - ETA: 12:06 - loss: 0.48 - ETA: 11:56 - loss: 0.47 - ETA: 11:46 - loss: 0.47 - ETA: 11:37 - loss: 0.47 - ETA: 11:27 - loss: 0.46 - ETA: 11:17 - loss: 0.46 - ETA: 11:08 - loss: 0.46 - ETA: 10:58 - loss: 0.45 - ETA: 10:48 - loss: 0.45 - ETA: 10:39 - loss: 0.44 - ETA: 10:29 - loss: 0.44 - ETA: 10:19 - loss: 0.44 - ETA: 10:10 - loss: 0.44 - ETA: 10:00 - loss: 0.43 - ETA: 9:50 - loss: 0.4340 - ETA: 9:41 - loss: 0.431 - ETA: 9:31 - loss: 0.428 - ETA: 9:21 - loss: 0.425 - ETA: 9:12 - loss: 0.422 - ETA: 9:02 - loss: 0.420 - ETA: 8:52 - loss: 0.417 - ETA: 8:43 - loss: 0.414 - ETA: 8:33 - loss: 0.412 - ETA: 8:24 - loss: 0.409 - ETA: 8:14 - loss: 0.407 - ETA: 8:04 - loss: 0.404 - ETA: 7:55 - loss: 0.402 - ETA: 7:45 - loss: 0.399 - ETA: 7:35 - loss: 0.397 - ETA: 7:26 - loss: 0.395 - ETA: 7:16 - loss: 0.393 - ETA: 7:06 - loss: 0.390 - ETA: 6:57 - loss: 0.388 - ETA: 6:47 - loss: 0.386 - ETA: 6:37 - loss: 0.384 - ETA: 6:28 - loss: 0.382 - ETA: 6:18 - loss: 0.380 - ETA: 6:09 - loss: 0.379 - ETA: 5:59 - loss: 0.377 - ETA: 5:49 - loss: 0.375 - ETA: 5:40 - loss: 0.373 - ETA: 5:30 - loss: 0.371 - ETA: 5:21 - loss: 0.369 - ETA: 5:11 - loss: 0.367 - ETA: 5:01 - loss: 0.366 - ETA: 4:52 - loss: 0.364 - ETA: 4:42 - loss: 0.362 - ETA: 4:32 - loss: 0.361 - ETA: 4:23 - loss: 0.359 - ETA: 4:13 - loss: 0.358 - ETA: 4:04 - loss: 0.356 - ETA: 3:54 - loss: 0.355 - ETA: 3:44 - loss: 0.353 - ETA: 3:35 - loss: 0.351 - ETA: 3:25 - loss: 0.350 - ETA: 3:15 - loss: 0.349 - ETA: 3:06 - loss: 0.347 - ETA: 2:56 - loss: 0.346 - ETA: 2:47 - loss: 0.344 - ETA: 2:37 - loss: 0.343 - ETA: 2:27 - loss: 0.341 - ETA: 2:18 - loss: 0.340 - ETA: 2:08 - loss: 0.339 - ETA: 1:58 - loss: 0.337 - ETA: 1:49 - loss: 0.336 - ETA: 1:39 - loss: 0.335 - ETA: 1:30 - loss: 0.334 - ETA: 1:20 - loss: 0.332 - ETA: 1:10 - loss: 0.331 - ETA: 1:01 - loss: 0.330 - ETA: 51s - loss: 0.329 - ETA: 42s - loss: 0.32 - ETA: 32s - loss: 0.32 - ETA: 22s - loss: 0.32 - ETA: 13s - loss: 0.32 - ETA: 3s - loss: 0.3234 - 1482s 8ms/step - loss: 0.3229 - val_loss: 0.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gstot\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "176853/176853 [==============================] - ETA: 23:19 - loss: 0.15 - ETA: 23:20 - loss: 0.14 - ETA: 23:50 - loss: 0.14 - ETA: 23:29 - loss: 0.14 - ETA: 23:12 - loss: 0.14 - ETA: 22:57 - loss: 0.14 - ETA: 22:43 - loss: 0.14 - ETA: 22:31 - loss: 0.14 - ETA: 22:20 - loss: 0.14 - ETA: 22:08 - loss: 0.14 - ETA: 21:58 - loss: 0.14 - ETA: 21:47 - loss: 0.14 - ETA: 21:36 - loss: 0.14 - ETA: 21:25 - loss: 0.14 - ETA: 21:15 - loss: 0.14 - ETA: 21:05 - loss: 0.14 - ETA: 20:54 - loss: 0.14 - ETA: 20:44 - loss: 0.14 - ETA: 20:33 - loss: 0.14 - ETA: 20:24 - loss: 0.14 - ETA: 20:14 - loss: 0.14 - ETA: 20:04 - loss: 0.14 - ETA: 19:54 - loss: 0.14 - ETA: 19:44 - loss: 0.14 - ETA: 19:34 - loss: 0.14 - ETA: 19:24 - loss: 0.14 - ETA: 19:14 - loss: 0.14 - ETA: 19:05 - loss: 0.14 - ETA: 18:55 - loss: 0.14 - ETA: 18:46 - loss: 0.14 - ETA: 18:36 - loss: 0.14 - ETA: 18:26 - loss: 0.14 - ETA: 18:17 - loss: 0.14 - ETA: 18:07 - loss: 0.14 - ETA: 17:57 - loss: 0.14 - ETA: 17:48 - loss: 0.14 - ETA: 17:38 - loss: 0.14 - ETA: 17:29 - loss: 0.14 - ETA: 17:19 - loss: 0.14 - ETA: 17:09 - loss: 0.14 - ETA: 16:59 - loss: 0.14 - ETA: 16:50 - loss: 0.14 - ETA: 16:40 - loss: 0.14 - ETA: 16:30 - loss: 0.14 - ETA: 16:21 - loss: 0.14 - ETA: 16:11 - loss: 0.14 - ETA: 16:01 - loss: 0.14 - ETA: 15:52 - loss: 0.14 - ETA: 15:42 - loss: 0.14 - ETA: 15:32 - loss: 0.14 - ETA: 15:23 - loss: 0.14 - ETA: 15:13 - loss: 0.14 - ETA: 15:04 - loss: 0.14 - ETA: 14:54 - loss: 0.14 - ETA: 14:44 - loss: 0.14 - ETA: 14:35 - loss: 0.14 - ETA: 14:25 - loss: 0.14 - ETA: 14:15 - loss: 0.14 - ETA: 14:06 - loss: 0.14 - ETA: 13:56 - loss: 0.14 - ETA: 13:46 - loss: 0.14 - ETA: 13:37 - loss: 0.14 - ETA: 13:27 - loss: 0.14 - ETA: 13:18 - loss: 0.14 - ETA: 13:08 - loss: 0.14 - ETA: 12:58 - loss: 0.14 - ETA: 12:49 - loss: 0.14 - ETA: 12:39 - loss: 0.14 - ETA: 12:30 - loss: 0.14 - ETA: 12:20 - loss: 0.14 - ETA: 12:10 - loss: 0.14 - ETA: 12:01 - loss: 0.14 - ETA: 11:51 - loss: 0.14 - ETA: 11:42 - loss: 0.14 - ETA: 11:32 - loss: 0.14 - ETA: 11:22 - loss: 0.14 - ETA: 11:13 - loss: 0.14 - ETA: 11:03 - loss: 0.14 - ETA: 10:54 - loss: 0.14 - ETA: 10:44 - loss: 0.14 - ETA: 10:34 - loss: 0.14 - ETA: 10:25 - loss: 0.14 - ETA: 10:15 - loss: 0.14 - ETA: 10:06 - loss: 0.14 - ETA: 9:56 - loss: 0.1430 - ETA: 9:46 - loss: 0.143 - ETA: 9:37 - loss: 0.143 - ETA: 9:27 - loss: 0.142 - ETA: 9:18 - loss: 0.142 - ETA: 9:08 - loss: 0.142 - ETA: 8:58 - loss: 0.142 - ETA: 8:49 - loss: 0.142 - ETA: 8:39 - loss: 0.142 - ETA: 8:30 - loss: 0.142 - ETA: 8:20 - loss: 0.142 - ETA: 8:11 - loss: 0.142 - ETA: 8:02 - loss: 0.142 - ETA: 7:52 - loss: 0.142 - ETA: 7:42 - loss: 0.142 - ETA: 7:33 - loss: 0.142 - ETA: 7:23 - loss: 0.142 - ETA: 7:14 - loss: 0.142 - ETA: 7:04 - loss: 0.142 - ETA: 6:55 - loss: 0.142 - ETA: 6:45 - loss: 0.142 - ETA: 6:35 - loss: 0.142 - ETA: 6:26 - loss: 0.142 - ETA: 6:16 - loss: 0.142 - ETA: 6:07 - loss: 0.142 - ETA: 5:57 - loss: 0.142 - ETA: 5:48 - loss: 0.142 - ETA: 5:38 - loss: 0.142 - ETA: 5:28 - loss: 0.142 - ETA: 5:19 - loss: 0.142 - ETA: 5:09 - loss: 0.142 - ETA: 5:00 - loss: 0.142 - ETA: 4:50 - loss: 0.142 - ETA: 4:41 - loss: 0.142 - ETA: 4:31 - loss: 0.142 - ETA: 4:21 - loss: 0.142 - ETA: 4:12 - loss: 0.142 - ETA: 4:02 - loss: 0.141 - ETA: 3:53 - loss: 0.141 - ETA: 3:43 - loss: 0.141 - ETA: 3:34 - loss: 0.142 - ETA: 3:24 - loss: 0.141 - ETA: 3:14 - loss: 0.141 - ETA: 3:05 - loss: 0.141 - ETA: 2:55 - loss: 0.141 - ETA: 2:46 - loss: 0.141 - ETA: 2:36 - loss: 0.141 - ETA: 2:27 - loss: 0.141 - ETA: 2:17 - loss: 0.141 - ETA: 2:07 - loss: 0.141 - ETA: 1:58 - loss: 0.141 - ETA: 1:48 - loss: 0.141 - ETA: 1:39 - loss: 0.141 - ETA: 1:29 - loss: 0.141 - ETA: 1:20 - loss: 0.141 - ETA: 1:10 - loss: 0.141 - ETA: 1:00 - loss: 0.141 - ETA: 51s - loss: 0.141 - ETA: 41s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 3s - loss: 0.1412 - 1475s 8ms/step - loss: 0.1412 - val_loss: 0.1474\n",
      "Epoch 3/7\n",
      "176853/176853 [==============================] - ETA: 23:17 - loss: 0.10 - ETA: 23:17 - loss: 0.11 - ETA: 23:02 - loss: 0.11 - ETA: 22:50 - loss: 0.11 - ETA: 22:40 - loss: 0.11 - ETA: 22:28 - loss: 0.11 - ETA: 22:19 - loss: 0.11 - ETA: 22:09 - loss: 0.11 - ETA: 21:59 - loss: 0.11 - ETA: 21:49 - loss: 0.11 - ETA: 21:39 - loss: 0.11 - ETA: 21:29 - loss: 0.11 - ETA: 21:20 - loss: 0.11 - ETA: 21:10 - loss: 0.11 - ETA: 21:00 - loss: 0.11 - ETA: 20:51 - loss: 0.11 - ETA: 20:41 - loss: 0.11 - ETA: 20:31 - loss: 0.11 - ETA: 20:21 - loss: 0.11 - ETA: 20:12 - loss: 0.11 - ETA: 20:03 - loss: 0.11 - ETA: 19:53 - loss: 0.11 - ETA: 19:43 - loss: 0.11 - ETA: 19:34 - loss: 0.11 - ETA: 19:24 - loss: 0.11 - ETA: 19:15 - loss: 0.11 - ETA: 19:05 - loss: 0.11 - ETA: 18:56 - loss: 0.11 - ETA: 18:46 - loss: 0.11 - ETA: 18:37 - loss: 0.11 - ETA: 18:27 - loss: 0.11 - ETA: 18:18 - loss: 0.11 - ETA: 18:08 - loss: 0.11 - ETA: 17:59 - loss: 0.11 - ETA: 17:49 - loss: 0.11 - ETA: 17:40 - loss: 0.11 - ETA: 17:34 - loss: 0.11 - ETA: 17:25 - loss: 0.11 - ETA: 17:15 - loss: 0.11 - ETA: 17:05 - loss: 0.11 - ETA: 16:56 - loss: 0.11 - ETA: 16:46 - loss: 0.11 - ETA: 16:37 - loss: 0.11 - ETA: 16:27 - loss: 0.11 - ETA: 16:17 - loss: 0.11 - ETA: 16:08 - loss: 0.11 - ETA: 15:58 - loss: 0.11 - ETA: 15:49 - loss: 0.11 - ETA: 15:39 - loss: 0.11 - ETA: 15:30 - loss: 0.11 - ETA: 15:20 - loss: 0.11 - ETA: 15:10 - loss: 0.11 - ETA: 15:01 - loss: 0.11 - ETA: 14:51 - loss: 0.11 - ETA: 14:42 - loss: 0.11 - ETA: 14:32 - loss: 0.11 - ETA: 14:23 - loss: 0.11 - ETA: 14:13 - loss: 0.11 - ETA: 14:04 - loss: 0.11 - ETA: 13:54 - loss: 0.11 - ETA: 13:45 - loss: 0.11 - ETA: 13:35 - loss: 0.11 - ETA: 13:25 - loss: 0.11 - ETA: 13:16 - loss: 0.11 - ETA: 13:06 - loss: 0.11 - ETA: 12:57 - loss: 0.11 - ETA: 12:47 - loss: 0.11 - ETA: 12:38 - loss: 0.11 - ETA: 12:28 - loss: 0.11 - ETA: 12:18 - loss: 0.11 - ETA: 12:09 - loss: 0.11 - ETA: 11:59 - loss: 0.11 - ETA: 11:50 - loss: 0.11 - ETA: 11:40 - loss: 0.11 - ETA: 11:30 - loss: 0.11 - ETA: 11:21 - loss: 0.11 - ETA: 11:11 - loss: 0.11 - ETA: 11:01 - loss: 0.11 - ETA: 10:52 - loss: 0.11 - ETA: 10:42 - loss: 0.11 - ETA: 10:33 - loss: 0.11 - ETA: 10:23 - loss: 0.11 - ETA: 10:13 - loss: 0.11 - ETA: 10:04 - loss: 0.11 - ETA: 9:54 - loss: 0.1139 - ETA: 9:45 - loss: 0.113 - ETA: 9:35 - loss: 0.113 - ETA: 9:26 - loss: 0.113 - ETA: 9:16 - loss: 0.113 - ETA: 9:06 - loss: 0.114 - ETA: 8:57 - loss: 0.114 - ETA: 8:47 - loss: 0.114 - ETA: 8:38 - loss: 0.114 - ETA: 8:28 - loss: 0.114 - ETA: 8:19 - loss: 0.114 - ETA: 8:09 - loss: 0.114 - ETA: 8:00 - loss: 0.114 - ETA: 7:50 - loss: 0.114 - ETA: 7:40 - loss: 0.114 - ETA: 7:31 - loss: 0.114 - ETA: 7:21 - loss: 0.114 - ETA: 7:12 - loss: 0.114 - ETA: 7:02 - loss: 0.114 - ETA: 6:53 - loss: 0.114 - ETA: 6:43 - loss: 0.114 - ETA: 6:34 - loss: 0.114 - ETA: 6:24 - loss: 0.114 - ETA: 6:15 - loss: 0.114 - ETA: 6:05 - loss: 0.114 - ETA: 5:55 - loss: 0.114 - ETA: 5:46 - loss: 0.114 - ETA: 5:36 - loss: 0.114 - ETA: 5:27 - loss: 0.114 - ETA: 5:17 - loss: 0.114 - ETA: 5:08 - loss: 0.114 - ETA: 4:58 - loss: 0.114 - ETA: 4:49 - loss: 0.114 - ETA: 4:39 - loss: 0.114 - ETA: 4:30 - loss: 0.114 - ETA: 4:20 - loss: 0.114 - ETA: 4:11 - loss: 0.114 - ETA: 4:01 - loss: 0.114 - ETA: 3:52 - loss: 0.114 - ETA: 3:42 - loss: 0.114 - ETA: 3:32 - loss: 0.114 - ETA: 3:23 - loss: 0.114 - ETA: 3:13 - loss: 0.114 - ETA: 3:04 - loss: 0.114 - ETA: 2:54 - loss: 0.114 - ETA: 2:45 - loss: 0.114 - ETA: 2:35 - loss: 0.115 - ETA: 2:26 - loss: 0.115 - ETA: 2:16 - loss: 0.115 - ETA: 2:07 - loss: 0.115 - ETA: 1:57 - loss: 0.115 - ETA: 1:48 - loss: 0.115 - ETA: 1:38 - loss: 0.115 - ETA: 1:29 - loss: 0.115 - ETA: 1:19 - loss: 0.115 - ETA: 1:10 - loss: 0.115 - ETA: 1:00 - loss: 0.115 - ETA: 51s - loss: 0.115 - ETA: 41s - loss: 0.11 - ETA: 32s - loss: 0.11 - ETA: 22s - loss: 0.11 - ETA: 13s - loss: 0.11 - ETA: 3s - loss: 0.1154 - 1468s 8ms/step - loss: 0.1154 - val_loss: 0.1493\n",
      "Epoch 4/7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176853/176853 [==============================] - ETA: 23:07 - loss: 0.08 - ETA: 22:59 - loss: 0.08 - ETA: 22:48 - loss: 0.08 - ETA: 22:44 - loss: 0.08 - ETA: 22:36 - loss: 0.08 - ETA: 22:26 - loss: 0.08 - ETA: 22:15 - loss: 0.08 - ETA: 22:04 - loss: 0.08 - ETA: 21:55 - loss: 0.08 - ETA: 21:46 - loss: 0.08 - ETA: 21:36 - loss: 0.08 - ETA: 21:26 - loss: 0.08 - ETA: 21:16 - loss: 0.08 - ETA: 21:06 - loss: 0.08 - ETA: 20:56 - loss: 0.08 - ETA: 20:46 - loss: 0.08 - ETA: 20:37 - loss: 0.08 - ETA: 20:27 - loss: 0.08 - ETA: 20:18 - loss: 0.08 - ETA: 20:08 - loss: 0.08 - ETA: 19:58 - loss: 0.08 - ETA: 19:49 - loss: 0.08 - ETA: 19:39 - loss: 0.08 - ETA: 19:29 - loss: 0.08 - ETA: 19:20 - loss: 0.08 - ETA: 19:10 - loss: 0.08 - ETA: 19:00 - loss: 0.08 - ETA: 18:51 - loss: 0.08 - ETA: 18:41 - loss: 0.08 - ETA: 18:32 - loss: 0.08 - ETA: 18:23 - loss: 0.08 - ETA: 18:13 - loss: 0.08 - ETA: 18:04 - loss: 0.08 - ETA: 17:54 - loss: 0.08 - ETA: 17:45 - loss: 0.08 - ETA: 17:35 - loss: 0.08 - ETA: 17:26 - loss: 0.08 - ETA: 17:17 - loss: 0.08 - ETA: 17:07 - loss: 0.08 - ETA: 16:57 - loss: 0.08 - ETA: 16:48 - loss: 0.08 - ETA: 16:39 - loss: 0.08 - ETA: 16:29 - loss: 0.08 - ETA: 16:20 - loss: 0.08 - ETA: 16:10 - loss: 0.08 - ETA: 16:00 - loss: 0.08 - ETA: 15:51 - loss: 0.08 - ETA: 15:41 - loss: 0.08 - ETA: 15:32 - loss: 0.08 - ETA: 15:22 - loss: 0.08 - ETA: 15:13 - loss: 0.08 - ETA: 15:03 - loss: 0.08 - ETA: 14:54 - loss: 0.08 - ETA: 14:44 - loss: 0.08 - ETA: 14:35 - loss: 0.08 - ETA: 14:25 - loss: 0.08 - ETA: 14:16 - loss: 0.08 - ETA: 14:07 - loss: 0.08 - ETA: 13:57 - loss: 0.08 - ETA: 13:48 - loss: 0.08 - ETA: 13:38 - loss: 0.08 - ETA: 13:29 - loss: 0.08 - ETA: 13:19 - loss: 0.08 - ETA: 13:10 - loss: 0.08 - ETA: 13:00 - loss: 0.08 - ETA: 12:51 - loss: 0.08 - ETA: 12:41 - loss: 0.08 - ETA: 12:32 - loss: 0.08 - ETA: 12:24 - loss: 0.08 - ETA: 12:15 - loss: 0.08 - ETA: 12:05 - loss: 0.08 - ETA: 11:56 - loss: 0.08 - ETA: 11:46 - loss: 0.08 - ETA: 11:36 - loss: 0.08 - ETA: 11:27 - loss: 0.08 - ETA: 11:17 - loss: 0.08 - ETA: 11:08 - loss: 0.08 - ETA: 10:59 - loss: 0.08 - ETA: 10:49 - loss: 0.08 - ETA: 10:40 - loss: 0.08 - ETA: 10:30 - loss: 0.08 - ETA: 10:21 - loss: 0.08 - ETA: 10:11 - loss: 0.08 - ETA: 10:02 - loss: 0.08 - ETA: 9:52 - loss: 0.0890 - ETA: 9:43 - loss: 0.089 - ETA: 9:33 - loss: 0.089 - ETA: 9:24 - loss: 0.089 - ETA: 9:14 - loss: 0.089 - ETA: 9:05 - loss: 0.089 - ETA: 8:55 - loss: 0.089 - ETA: 8:46 - loss: 0.089 - ETA: 8:36 - loss: 0.089 - ETA: 8:27 - loss: 0.089 - ETA: 8:17 - loss: 0.089 - ETA: 8:08 - loss: 0.089 - ETA: 7:58 - loss: 0.089 - ETA: 7:49 - loss: 0.089 - ETA: 7:39 - loss: 0.089 - ETA: 7:30 - loss: 0.089 - ETA: 7:20 - loss: 0.089 - ETA: 7:11 - loss: 0.089 - ETA: 7:01 - loss: 0.089 - ETA: 6:52 - loss: 0.089 - ETA: 6:42 - loss: 0.089 - ETA: 6:33 - loss: 0.089 - ETA: 6:23 - loss: 0.090 - ETA: 6:14 - loss: 0.090 - ETA: 6:04 - loss: 0.090 - ETA: 5:55 - loss: 0.090 - ETA: 5:45 - loss: 0.090 - ETA: 5:36 - loss: 0.090 - ETA: 5:27 - loss: 0.090 - ETA: 5:17 - loss: 0.090 - ETA: 5:08 - loss: 0.090 - ETA: 4:58 - loss: 0.090 - ETA: 4:49 - loss: 0.090 - ETA: 4:39 - loss: 0.090 - ETA: 4:30 - loss: 0.090 - ETA: 4:20 - loss: 0.090 - ETA: 4:11 - loss: 0.090 - ETA: 4:01 - loss: 0.090 - ETA: 3:52 - loss: 0.090 - ETA: 3:42 - loss: 0.090 - ETA: 3:33 - loss: 0.090 - ETA: 3:23 - loss: 0.090 - ETA: 3:14 - loss: 0.090 - ETA: 3:04 - loss: 0.091 - ETA: 2:55 - loss: 0.091 - ETA: 2:45 - loss: 0.091 - ETA: 2:36 - loss: 0.091 - ETA: 2:27 - loss: 0.091 - ETA: 2:17 - loss: 0.091 - ETA: 2:08 - loss: 0.091 - ETA: 1:58 - loss: 0.091 - ETA: 1:48 - loss: 0.091 - ETA: 1:39 - loss: 0.091 - ETA: 1:29 - loss: 0.091 - ETA: 1:20 - loss: 0.091 - ETA: 1:10 - loss: 0.091 - ETA: 1:01 - loss: 0.091 - ETA: 51s - loss: 0.091 - ETA: 41s - loss: 0.09 - ETA: 32s - loss: 0.09 - ETA: 22s - loss: 0.09 - ETA: 13s - loss: 0.09 - ETA: 3s - loss: 0.0917 - 1481s 8ms/step - loss: 0.0917 - val_loss: 0.1569\n",
      "Epoch 5/7\n",
      "176400/176853 [============================>.] - ETA: 23:59 - loss: 0.06 - ETA: 23:42 - loss: 0.06 - ETA: 24:05 - loss: 0.06 - ETA: 23:38 - loss: 0.06 - ETA: 23:34 - loss: 0.06 - ETA: 23:37 - loss: 0.06 - ETA: 24:03 - loss: 0.06 - ETA: 24:16 - loss: 0.06 - ETA: 24:22 - loss: 0.06 - ETA: 24:09 - loss: 0.06 - ETA: 23:56 - loss: 0.06 - ETA: 23:39 - loss: 0.06 - ETA: 23:20 - loss: 0.06 - ETA: 23:02 - loss: 0.06 - ETA: 22:47 - loss: 0.06 - ETA: 22:35 - loss: 0.06 - ETA: 22:19 - loss: 0.06 - ETA: 22:08 - loss: 0.06 - ETA: 21:54 - loss: 0.06 - ETA: 21:40 - loss: 0.06 - ETA: 21:26 - loss: 0.06 - ETA: 21:16 - loss: 0.06 - ETA: 21:05 - loss: 0.06 - ETA: 20:53 - loss: 0.06 - ETA: 20:42 - loss: 0.06 - ETA: 20:33 - loss: 0.06 - ETA: 20:22 - loss: 0.06 - ETA: 20:13 - loss: 0.06 - ETA: 20:06 - loss: 0.06 - ETA: 19:55 - loss: 0.06 - ETA: 19:43 - loss: 0.06 - ETA: 19:31 - loss: 0.06 - ETA: 19:20 - loss: 0.06 - ETA: 19:08 - loss: 0.06 - ETA: 18:57 - loss: 0.06 - ETA: 18:46 - loss: 0.06 - ETA: 18:36 - loss: 0.06 - ETA: 18:25 - loss: 0.06 - ETA: 18:16 - loss: 0.06 - ETA: 18:06 - loss: 0.06 - ETA: 17:55 - loss: 0.06 - ETA: 17:45 - loss: 0.06 - ETA: 17:38 - loss: 0.06 - ETA: 17:28 - loss: 0.06 - ETA: 17:18 - loss: 0.06 - ETA: 17:07 - loss: 0.06 - ETA: 16:59 - loss: 0.06 - ETA: 16:49 - loss: 0.06 - ETA: 16:40 - loss: 0.06 - ETA: 16:31 - loss: 0.06 - ETA: 16:21 - loss: 0.06 - ETA: 16:11 - loss: 0.06 - ETA: 16:01 - loss: 0.06 - ETA: 15:50 - loss: 0.06 - ETA: 15:39 - loss: 0.06 - ETA: 15:28 - loss: 0.06 - ETA: 15:17 - loss: 0.06 - ETA: 15:06 - loss: 0.06 - ETA: 14:56 - loss: 0.06 - ETA: 14:45 - loss: 0.06 - ETA: 14:34 - loss: 0.06 - ETA: 14:25 - loss: 0.06 - ETA: 14:15 - loss: 0.06 - ETA: 14:05 - loss: 0.06 - ETA: 13:54 - loss: 0.06 - ETA: 13:43 - loss: 0.06 - ETA: 13:34 - loss: 0.06 - ETA: 13:24 - loss: 0.06 - ETA: 13:14 - loss: 0.06 - ETA: 13:03 - loss: 0.06 - ETA: 12:53 - loss: 0.06 - ETA: 12:42 - loss: 0.06 - ETA: 12:32 - loss: 0.06 - ETA: 12:22 - loss: 0.06 - ETA: 12:11 - loss: 0.06 - ETA: 12:02 - loss: 0.06 - ETA: 11:52 - loss: 0.06 - ETA: 11:43 - loss: 0.06 - ETA: 11:33 - loss: 0.06 - ETA: 11:24 - loss: 0.06 - ETA: 11:14 - loss: 0.06 - ETA: 11:04 - loss: 0.06 - ETA: 10:54 - loss: 0.06 - ETA: 10:44 - loss: 0.06 - ETA: 10:33 - loss: 0.06 - ETA: 10:23 - loss: 0.06 - ETA: 10:13 - loss: 0.06 - ETA: 10:03 - loss: 0.06 - ETA: 9:54 - loss: 0.0655 - ETA: 9:45 - loss: 0.065 - ETA: 9:34 - loss: 0.065 - ETA: 9:24 - loss: 0.065 - ETA: 9:14 - loss: 0.065 - ETA: 9:04 - loss: 0.065 - ETA: 8:54 - loss: 0.065 - ETA: 8:44 - loss: 0.065 - ETA: 8:34 - loss: 0.066 - ETA: 8:24 - loss: 0.066 - ETA: 8:14 - loss: 0.066 - ETA: 8:04 - loss: 0.066 - ETA: 7:54 - loss: 0.066 - ETA: 7:43 - loss: 0.066 - ETA: 7:34 - loss: 0.066 - ETA: 7:23 - loss: 0.066 - ETA: 7:13 - loss: 0.066 - ETA: 7:03 - loss: 0.066 - ETA: 6:53 - loss: 0.066 - ETA: 6:43 - loss: 0.066 - ETA: 6:33 - loss: 0.066 - ETA: 6:23 - loss: 0.066 - ETA: 6:13 - loss: 0.066 - ETA: 6:03 - loss: 0.066 - ETA: 5:53 - loss: 0.067 - ETA: 5:42 - loss: 0.067 - ETA: 5:32 - loss: 0.067 - ETA: 5:22 - loss: 0.067 - ETA: 5:11 - loss: 0.067 - ETA: 5:01 - loss: 0.067 - ETA: 4:51 - loss: 0.067 - ETA: 4:41 - loss: 0.067 - ETA: 4:31 - loss: 0.067 - ETA: 4:21 - loss: 0.067 - ETA: 4:10 - loss: 0.067 - ETA: 4:00 - loss: 0.067 - ETA: 3:50 - loss: 0.067 - ETA: 3:40 - loss: 0.067 - ETA: 3:30 - loss: 0.068 - ETA: 3:19 - loss: 0.068 - ETA: 3:09 - loss: 0.068 - ETA: 2:59 - loss: 0.068 - ETA: 2:49 - loss: 0.068 - ETA: 2:38 - loss: 0.068 - ETA: 2:28 - loss: 0.068 - ETA: 2:18 - loss: 0.068 - ETA: 2:07 - loss: 0.068 - ETA: 1:57 - loss: 0.068 - ETA: 1:47 - loss: 0.068 - ETA: 1:37 - loss: 0.068 - ETA: 1:26 - loss: 0.068 - ETA: 1:16 - loss: 0.068 - ETA: 1:05 - loss: 0.069 - ETA: 55s - loss: 0.069 - ETA: 45s - loss: 0.06 - ETA: 34s - loss: 0.06 - ETA: 24s - loss: 0.06 - ETA: 14s - loss: 0.06 - ETA: 3s - loss: 0.0693 "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=issue_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
