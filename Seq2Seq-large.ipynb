{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 290,028 rows 18 columns\n",
      "Test: 39,550 rows 18 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1118279</th>\n",
       "      <td>02/13/2018</td>\n",
       "      <td>Credit reporting, credit repair services, or other personal consumer reports</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Unable to get your credit report or credit score</td>\n",
       "      <td>Other problem getting your report or credit score</td>\n",
       "      <td>I AM UNABLE TO OBATIN MY EQUIFAX CREDIT FILE OR SCORE. \\nPREVIOUSLY I WAS ABLE TO PULL MY REPORT UP UNTIL XX/XX/XXXX THEN ON XX/XX/XXXX I TRYED REQUESTING MY FILE TROUGH XXXX AND I WAS ADVISED BY BOTH XXXX AND EQUIFAX THAT MY FILE IS MISSING MY SOCIAL SECURITY NUMBER AND FOR THAT REASON I AM UNABLE TO OBTAIN MY REPORT! \\n\\nI SPOKE TO THEM SEVERAL TIMES. \\nEQUIFAX HAS BEEN GIVING ME THE RUNAROUND THE HAVE BEEN RUDE UNPROFESSIONAL AND UNCOMFORTABLE. I WAS ADVISED BY GLOBAL EQUIFAX ON XX/XX/XXX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>FL</td>\n",
       "      <td>331XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>02/13/2018</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2813030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435082</th>\n",
       "      <td>06/23/2015</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>Making/receiving payments, sending money</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hello i have filed a complain about wells fargo with cfpb complain # XXXX and explained that even though a let them know that reason i was canceling my debt card with them was that XXXX rent a car was attempting to charge my card something i dont owe and canceled the card befor they actuly charged and still wells fargo paid those fradulant chargesthey did not do anything and know this company continues to charge my card as of XX/XX/XXXX XXXXwith no reason please help me my account is about t...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>WELLS FARGO &amp; COMPANY</td>\n",
       "      <td>GA</td>\n",
       "      <td>300XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>06/23/2015</td>\n",
       "      <td>Closed with monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1434805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103519</th>\n",
       "      <td>08/04/2015</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I do not know</td>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Frequent or repeated calls</td>\n",
       "      <td>They say they are from Fitzgerald and associates XXXX. They will not tell me what it about just that i owe money. They refuse to create paper trail or provide information. They will not provide me anything in writing. They threatened to take my car away. They called me on my cell phone at XXXX on XXXX. They called me at work on XXXX and i asked them not to call me here. They called again on XXXX i asked them not to call me again.</td>\n",
       "      <td>Company believes it acted appropriately as authorized by contract or law</td>\n",
       "      <td>Fitzgerald Goldman &amp; Associates, Inc.</td>\n",
       "      <td>WI</td>\n",
       "      <td>531XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>08/04/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1501032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date received  \\\n",
       "1118279    02/13/2018   \n",
       "435082     06/23/2015   \n",
       "1103519    08/04/2015   \n",
       "\n",
       "                                                                              Product  \\\n",
       "1118279  Credit reporting, credit repair services, or other personal consumer reports   \n",
       "435082                                                        Bank account or service   \n",
       "1103519                                                               Debt collection   \n",
       "\n",
       "              Sub-product                                             Issue  \\\n",
       "1118279  Credit reporting  Unable to get your credit report or credit score   \n",
       "435082   Checking account          Making/receiving payments, sending money   \n",
       "1103519     I do not know                             Communication tactics   \n",
       "\n",
       "                                                 Sub-issue  \\\n",
       "1118279  Other problem getting your report or credit score   \n",
       "435082                                                 NaN   \n",
       "1103519                         Frequent or repeated calls   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Consumer complaint narrative  \\\n",
       "1118279  I AM UNABLE TO OBATIN MY EQUIFAX CREDIT FILE OR SCORE. \\nPREVIOUSLY I WAS ABLE TO PULL MY REPORT UP UNTIL XX/XX/XXXX THEN ON XX/XX/XXXX I TRYED REQUESTING MY FILE TROUGH XXXX AND I WAS ADVISED BY BOTH XXXX AND EQUIFAX THAT MY FILE IS MISSING MY SOCIAL SECURITY NUMBER AND FOR THAT REASON I AM UNABLE TO OBTAIN MY REPORT! \\n\\nI SPOKE TO THEM SEVERAL TIMES. \\nEQUIFAX HAS BEEN GIVING ME THE RUNAROUND THE HAVE BEEN RUDE UNPROFESSIONAL AND UNCOMFORTABLE. I WAS ADVISED BY GLOBAL EQUIFAX ON XX/XX/XXX...   \n",
       "435082   hello i have filed a complain about wells fargo with cfpb complain # XXXX and explained that even though a let them know that reason i was canceling my debt card with them was that XXXX rent a car was attempting to charge my card something i dont owe and canceled the card befor they actuly charged and still wells fargo paid those fradulant chargesthey did not do anything and know this company continues to charge my card as of XX/XX/XXXX XXXXwith no reason please help me my account is about t...   \n",
       "1103519                                                                    They say they are from Fitzgerald and associates XXXX. They will not tell me what it about just that i owe money. They refuse to create paper trail or provide information. They will not provide me anything in writing. They threatened to take my car away. They called me on my cell phone at XXXX on XXXX. They called me at work on XXXX and i asked them not to call me here. They called again on XXXX i asked them not to call me again.   \n",
       "\n",
       "                                                          Company public response  \\\n",
       "1118279                                                                       NaN   \n",
       "435082                           Company chooses not to provide a public response   \n",
       "1103519  Company believes it acted appropriately as authorized by contract or law   \n",
       "\n",
       "                                       Company State ZIP code Tags  \\\n",
       "1118279                          EQUIFAX, INC.    FL    331XX  NaN   \n",
       "435082                   WELLS FARGO & COMPANY    GA    300XX  NaN   \n",
       "1103519  Fitzgerald Goldman & Associates, Inc.    WI    531XX  NaN   \n",
       "\n",
       "        Consumer consent provided? Submitted via Date sent to company  \\\n",
       "1118279           Consent provided           Web           02/13/2018   \n",
       "435082            Consent provided           Web           06/23/2015   \n",
       "1103519           Consent provided           Web           08/04/2015   \n",
       "\n",
       "        Company response to consumer Timely response? Consumer disputed?  \\\n",
       "1118279      Closed with explanation              Yes                NaN   \n",
       "435082   Closed with monetary relief              Yes                 No   \n",
       "1103519      Closed with explanation               No                Yes   \n",
       "\n",
       "         Complaint ID  \n",
       "1118279       2813030  \n",
       "435082        1434805  \n",
       "1103519       1501032  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training and test\n",
    "data = pd.read_csv('consumer_complaints.csv')\n",
    "pData = data.dropna(subset=[\"Consumer complaint narrative\", \"Issue\"])\n",
    "traindf, testdf = train_test_split(pData, \n",
    "                                   test_size=.12)\n",
    "# print data sizes\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRawNarrative = traindf['Consumer complaint narrative'].tolist()\n",
    "trainIssueRaw = traindf['Issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 256 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 24 sec\n",
      "WARNING:root:Finished parsing 290,028 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 17 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=144)\n",
    "train_body_vecs = body_pp.fit_transform(trainRawNarrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 18 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 1 sec\n",
      "WARNING:root:Finished parsing 290,028 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 1 sec\n"
     ]
    }
   ],
   "source": [
    "issue_pp = processor(append_indicators=True, keep_n=9000, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_issue_vecs = issue_pp.fit_transform(trainIssueRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('issue_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(issue_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_issue_vecs.npy', train_issue_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (290028, 144)\n",
      "Shape of decoder input: (290028, 11)\n",
      "Shape of decoder target: (290028, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_issue_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8,002\n",
      "Size of vocabulary for issue_pp.dpkl: 244\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('issue_pp.dpkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    73200       Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 144)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 244)    73444       Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 3,632,644\n",
      "Trainable params: 3,630,844\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "#viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255224 samples, validate on 34804 samples\n",
      "Epoch 1/7\n",
      "255224/255224 [==============================] - ETA: 1:20:45 - loss: 6.27 - ETA: 57:28 - loss: 5.0477 - ETA: 49:26 - loss: 4.31 - ETA: 45:21 - loss: 3.78 - ETA: 43:06 - loss: 3.39 - ETA: 41:25 - loss: 3.08 - ETA: 40:09 - loss: 2.82 - ETA: 39:11 - loss: 2.61 - ETA: 38:22 - loss: 2.43 - ETA: 37:40 - loss: 2.28 - ETA: 37:05 - loss: 2.14 - ETA: 36:34 - loss: 2.02 - ETA: 36:08 - loss: 1.92 - ETA: 35:44 - loss: 1.83 - ETA: 35:20 - loss: 1.75 - ETA: 35:01 - loss: 1.68 - ETA: 34:40 - loss: 1.61 - ETA: 34:20 - loss: 1.55 - ETA: 34:01 - loss: 1.50 - ETA: 33:43 - loss: 1.45 - ETA: 33:25 - loss: 1.40 - ETA: 33:09 - loss: 1.36 - ETA: 32:54 - loss: 1.32 - ETA: 32:39 - loss: 1.28 - ETA: 32:24 - loss: 1.25 - ETA: 32:10 - loss: 1.22 - ETA: 31:55 - loss: 1.19 - ETA: 31:41 - loss: 1.16 - ETA: 31:27 - loss: 1.13 - ETA: 31:14 - loss: 1.11 - ETA: 31:01 - loss: 1.09 - ETA: 30:48 - loss: 1.06 - ETA: 30:35 - loss: 1.04 - ETA: 30:22 - loss: 1.03 - ETA: 30:10 - loss: 1.01 - ETA: 29:58 - loss: 0.99 - ETA: 29:46 - loss: 0.97 - ETA: 29:34 - loss: 0.96 - ETA: 29:22 - loss: 0.94 - ETA: 29:10 - loss: 0.93 - ETA: 28:58 - loss: 0.91 - ETA: 28:47 - loss: 0.90 - ETA: 28:35 - loss: 0.88 - ETA: 28:25 - loss: 0.87 - ETA: 28:14 - loss: 0.86 - ETA: 28:02 - loss: 0.85 - ETA: 27:51 - loss: 0.84 - ETA: 27:40 - loss: 0.82 - ETA: 27:29 - loss: 0.81 - ETA: 27:18 - loss: 0.80 - ETA: 27:07 - loss: 0.79 - ETA: 26:56 - loss: 0.78 - ETA: 26:45 - loss: 0.77 - ETA: 26:35 - loss: 0.76 - ETA: 26:24 - loss: 0.76 - ETA: 26:14 - loss: 0.75 - ETA: 26:03 - loss: 0.74 - ETA: 25:53 - loss: 0.73 - ETA: 25:43 - loss: 0.72 - ETA: 25:32 - loss: 0.72 - ETA: 25:21 - loss: 0.71 - ETA: 25:11 - loss: 0.70 - ETA: 25:00 - loss: 0.69 - ETA: 24:49 - loss: 0.69 - ETA: 24:39 - loss: 0.68 - ETA: 24:29 - loss: 0.67 - ETA: 24:18 - loss: 0.67 - ETA: 24:08 - loss: 0.66 - ETA: 23:57 - loss: 0.66 - ETA: 23:47 - loss: 0.65 - ETA: 23:37 - loss: 0.64 - ETA: 23:26 - loss: 0.64 - ETA: 23:16 - loss: 0.63 - ETA: 23:06 - loss: 0.63 - ETA: 22:55 - loss: 0.62 - ETA: 22:45 - loss: 0.62 - ETA: 22:35 - loss: 0.61 - ETA: 22:25 - loss: 0.61 - ETA: 22:15 - loss: 0.60 - ETA: 22:05 - loss: 0.60 - ETA: 21:54 - loss: 0.59 - ETA: 21:44 - loss: 0.59 - ETA: 21:34 - loss: 0.59 - ETA: 21:24 - loss: 0.58 - ETA: 21:16 - loss: 0.58 - ETA: 21:06 - loss: 0.57 - ETA: 20:56 - loss: 0.57 - ETA: 20:45 - loss: 0.56 - ETA: 20:35 - loss: 0.56 - ETA: 20:25 - loss: 0.56 - ETA: 20:15 - loss: 0.55 - ETA: 20:05 - loss: 0.55 - ETA: 19:55 - loss: 0.55 - ETA: 19:45 - loss: 0.54 - ETA: 19:34 - loss: 0.54 - ETA: 19:24 - loss: 0.54 - ETA: 19:14 - loss: 0.53 - ETA: 19:05 - loss: 0.53 - ETA: 18:56 - loss: 0.53 - ETA: 18:46 - loss: 0.52 - ETA: 18:35 - loss: 0.52 - ETA: 18:25 - loss: 0.52 - ETA: 18:15 - loss: 0.51 - ETA: 18:04 - loss: 0.51 - ETA: 17:54 - loss: 0.51 - ETA: 17:44 - loss: 0.51 - ETA: 17:33 - loss: 0.50 - ETA: 17:23 - loss: 0.50 - ETA: 17:13 - loss: 0.50 - ETA: 17:03 - loss: 0.49 - ETA: 16:52 - loss: 0.49 - ETA: 16:42 - loss: 0.49 - ETA: 16:32 - loss: 0.49 - ETA: 16:22 - loss: 0.48 - ETA: 16:12 - loss: 0.48 - ETA: 16:01 - loss: 0.48 - ETA: 15:51 - loss: 0.48 - ETA: 15:41 - loss: 0.47 - ETA: 15:31 - loss: 0.47 - ETA: 15:21 - loss: 0.47 - ETA: 15:10 - loss: 0.47 - ETA: 15:00 - loss: 0.47 - ETA: 14:50 - loss: 0.46 - ETA: 14:40 - loss: 0.46 - ETA: 14:30 - loss: 0.46 - ETA: 14:20 - loss: 0.46 - ETA: 14:10 - loss: 0.46 - ETA: 14:00 - loss: 0.45 - ETA: 13:49 - loss: 0.45 - ETA: 13:39 - loss: 0.45 - ETA: 13:29 - loss: 0.45 - ETA: 13:19 - loss: 0.45 - ETA: 13:09 - loss: 0.44 - ETA: 12:59 - loss: 0.44 - ETA: 12:49 - loss: 0.44 - ETA: 12:39 - loss: 0.44 - ETA: 12:29 - loss: 0.44 - ETA: 12:19 - loss: 0.44 - ETA: 12:09 - loss: 0.43 - ETA: 11:59 - loss: 0.43 - ETA: 11:49 - loss: 0.43 - ETA: 11:38 - loss: 0.43 - ETA: 11:28 - loss: 0.43 - ETA: 11:18 - loss: 0.43 - ETA: 11:08 - loss: 0.42 - ETA: 10:58 - loss: 0.42 - ETA: 10:48 - loss: 0.42 - ETA: 10:38 - loss: 0.42 - ETA: 10:29 - loss: 0.42 - ETA: 10:19 - loss: 0.42 - ETA: 10:09 - loss: 0.41 - ETA: 9:59 - loss: 0.4179 - ETA: 9:49 - loss: 0.416 - ETA: 9:39 - loss: 0.415 - ETA: 9:29 - loss: 0.413 - ETA: 9:19 - loss: 0.412 - ETA: 9:09 - loss: 0.410 - ETA: 8:59 - loss: 0.409 - ETA: 8:49 - loss: 0.408 - ETA: 8:39 - loss: 0.406 - ETA: 8:29 - loss: 0.405 - ETA: 8:19 - loss: 0.404 - ETA: 8:09 - loss: 0.403 - ETA: 7:59 - loss: 0.401 - ETA: 7:49 - loss: 0.400 - ETA: 7:39 - loss: 0.399 - ETA: 7:29 - loss: 0.398 - ETA: 7:19 - loss: 0.396 - ETA: 7:10 - loss: 0.395 - ETA: 7:00 - loss: 0.394 - ETA: 6:50 - loss: 0.393 - ETA: 6:40 - loss: 0.392 - ETA: 6:30 - loss: 0.391 - ETA: 6:20 - loss: 0.389 - ETA: 6:10 - loss: 0.388 - ETA: 6:00 - loss: 0.387 - ETA: 5:50 - loss: 0.386 - ETA: 5:41 - loss: 0.385 - ETA: 5:31 - loss: 0.384 - ETA: 5:21 - loss: 0.383 - ETA: 5:11 - loss: 0.382 - ETA: 5:01 - loss: 0.381 - ETA: 4:51 - loss: 0.380 - ETA: 4:42 - loss: 0.379 - ETA: 4:32 - loss: 0.378 - ETA: 4:22 - loss: 0.377 - ETA: 4:12 - loss: 0.376 - ETA: 4:02 - loss: 0.375 - ETA: 3:52 - loss: 0.374 - ETA: 3:42 - loss: 0.373 - ETA: 3:33 - loss: 0.372 - ETA: 3:23 - loss: 0.371 - ETA: 3:13 - loss: 0.370 - ETA: 3:03 - loss: 0.369 - ETA: 2:53 - loss: 0.368 - ETA: 2:43 - loss: 0.367 - ETA: 2:34 - loss: 0.366 - ETA: 2:24 - loss: 0.365 - ETA: 2:14 - loss: 0.365 - ETA: 2:04 - loss: 0.364 - ETA: 1:54 - loss: 0.363 - ETA: 1:44 - loss: 0.362 - ETA: 1:35 - loss: 0.361 - ETA: 1:25 - loss: 0.360 - ETA: 1:15 - loss: 0.359 - ETA: 1:05 - loss: 0.359 - ETA: 55s - loss: 0.358 - ETA: 45s - loss: 0.35 - ETA: 36s - loss: 0.35 - ETA: 26s - loss: 0.35 - ETA: 16s - loss: 0.35 - ETA: 6s - loss: 0.3542 - 2188s 9ms/step - loss: 0.3536 - val_loss: 0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gstot\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_1/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "255224/255224 [==============================] - ETA: 34:11 - loss: 0.17 - ETA: 33:57 - loss: 0.17 - ETA: 33:43 - loss: 0.17 - ETA: 33:30 - loss: 0.17 - ETA: 33:18 - loss: 0.17 - ETA: 33:10 - loss: 0.17 - ETA: 32:58 - loss: 0.16 - ETA: 32:48 - loss: 0.16 - ETA: 32:40 - loss: 0.16 - ETA: 32:29 - loss: 0.16 - ETA: 32:19 - loss: 0.17 - ETA: 32:13 - loss: 0.17 - ETA: 32:03 - loss: 0.17 - ETA: 31:55 - loss: 0.17 - ETA: 31:59 - loss: 0.17 - ETA: 31:50 - loss: 0.17 - ETA: 31:39 - loss: 0.17 - ETA: 31:29 - loss: 0.17 - ETA: 31:19 - loss: 0.17 - ETA: 31:08 - loss: 0.17 - ETA: 30:58 - loss: 0.17 - ETA: 30:50 - loss: 0.17 - ETA: 30:41 - loss: 0.17 - ETA: 30:33 - loss: 0.17 - ETA: 30:24 - loss: 0.17 - ETA: 30:15 - loss: 0.17 - ETA: 30:06 - loss: 0.17 - ETA: 29:57 - loss: 0.17 - ETA: 29:48 - loss: 0.17 - ETA: 29:40 - loss: 0.17 - ETA: 29:31 - loss: 0.17 - ETA: 29:22 - loss: 0.17 - ETA: 29:13 - loss: 0.17 - ETA: 29:04 - loss: 0.17 - ETA: 28:55 - loss: 0.17 - ETA: 28:46 - loss: 0.17 - ETA: 28:36 - loss: 0.17 - ETA: 28:27 - loss: 0.17 - ETA: 28:18 - loss: 0.17 - ETA: 28:09 - loss: 0.17 - ETA: 27:59 - loss: 0.17 - ETA: 27:50 - loss: 0.17 - ETA: 27:40 - loss: 0.17 - ETA: 27:31 - loss: 0.17 - ETA: 27:25 - loss: 0.17 - ETA: 27:16 - loss: 0.17 - ETA: 27:07 - loss: 0.17 - ETA: 26:57 - loss: 0.17 - ETA: 26:47 - loss: 0.17 - ETA: 26:38 - loss: 0.17 - ETA: 26:28 - loss: 0.17 - ETA: 26:18 - loss: 0.17 - ETA: 26:09 - loss: 0.17 - ETA: 25:59 - loss: 0.17 - ETA: 25:50 - loss: 0.17 - ETA: 25:40 - loss: 0.17 - ETA: 25:30 - loss: 0.17 - ETA: 25:20 - loss: 0.17 - ETA: 25:11 - loss: 0.17 - ETA: 25:01 - loss: 0.17 - ETA: 24:52 - loss: 0.17 - ETA: 24:42 - loss: 0.17 - ETA: 24:32 - loss: 0.17 - ETA: 24:23 - loss: 0.17 - ETA: 24:13 - loss: 0.17 - ETA: 24:03 - loss: 0.17 - ETA: 23:53 - loss: 0.17 - ETA: 23:44 - loss: 0.17 - ETA: 23:34 - loss: 0.17 - ETA: 23:24 - loss: 0.17 - ETA: 23:14 - loss: 0.17 - ETA: 23:05 - loss: 0.17 - ETA: 22:55 - loss: 0.17 - ETA: 22:45 - loss: 0.17 - ETA: 22:36 - loss: 0.17 - ETA: 22:26 - loss: 0.17 - ETA: 22:16 - loss: 0.17 - ETA: 22:06 - loss: 0.17 - ETA: 21:56 - loss: 0.17 - ETA: 21:46 - loss: 0.17 - ETA: 21:36 - loss: 0.17 - ETA: 21:26 - loss: 0.17 - ETA: 21:16 - loss: 0.17 - ETA: 21:06 - loss: 0.17 - ETA: 20:56 - loss: 0.17 - ETA: 20:46 - loss: 0.17 - ETA: 20:37 - loss: 0.17 - ETA: 20:27 - loss: 0.17 - ETA: 20:17 - loss: 0.17 - ETA: 20:07 - loss: 0.17 - ETA: 19:57 - loss: 0.17 - ETA: 19:47 - loss: 0.17 - ETA: 19:37 - loss: 0.17 - ETA: 19:28 - loss: 0.17 - ETA: 19:18 - loss: 0.17 - ETA: 19:08 - loss: 0.17 - ETA: 18:58 - loss: 0.17 - ETA: 18:48 - loss: 0.17 - ETA: 18:38 - loss: 0.17 - ETA: 18:29 - loss: 0.17 - ETA: 18:19 - loss: 0.17 - ETA: 18:09 - loss: 0.17 - ETA: 17:59 - loss: 0.17 - ETA: 17:49 - loss: 0.17 - ETA: 17:40 - loss: 0.17 - ETA: 17:30 - loss: 0.17 - ETA: 17:20 - loss: 0.17 - ETA: 17:10 - loss: 0.17 - ETA: 17:00 - loss: 0.17 - ETA: 16:50 - loss: 0.17 - ETA: 16:40 - loss: 0.17 - ETA: 16:30 - loss: 0.17 - ETA: 16:20 - loss: 0.16 - ETA: 16:11 - loss: 0.16 - ETA: 16:01 - loss: 0.16 - ETA: 15:51 - loss: 0.16 - ETA: 15:41 - loss: 0.16 - ETA: 15:31 - loss: 0.16 - ETA: 15:21 - loss: 0.17 - ETA: 15:12 - loss: 0.17 - ETA: 15:02 - loss: 0.16 - ETA: 14:52 - loss: 0.16 - ETA: 14:42 - loss: 0.16 - ETA: 14:32 - loss: 0.16 - ETA: 14:22 - loss: 0.16 - ETA: 14:12 - loss: 0.16 - ETA: 14:02 - loss: 0.16 - ETA: 13:53 - loss: 0.16 - ETA: 13:43 - loss: 0.16 - ETA: 13:33 - loss: 0.16 - ETA: 13:23 - loss: 0.16 - ETA: 13:13 - loss: 0.16 - ETA: 13:03 - loss: 0.16 - ETA: 12:53 - loss: 0.16 - ETA: 12:43 - loss: 0.16 - ETA: 12:34 - loss: 0.16 - ETA: 12:24 - loss: 0.16 - ETA: 12:14 - loss: 0.16 - ETA: 12:04 - loss: 0.16 - ETA: 11:54 - loss: 0.16 - ETA: 11:44 - loss: 0.16 - ETA: 11:34 - loss: 0.16 - ETA: 11:24 - loss: 0.16 - ETA: 11:14 - loss: 0.16 - ETA: 11:05 - loss: 0.16 - ETA: 10:55 - loss: 0.16 - ETA: 10:45 - loss: 0.16 - ETA: 10:35 - loss: 0.16 - ETA: 10:25 - loss: 0.16 - ETA: 10:15 - loss: 0.16 - ETA: 10:05 - loss: 0.16 - ETA: 9:55 - loss: 0.1694 - ETA: 9:45 - loss: 0.169 - ETA: 9:35 - loss: 0.169 - ETA: 9:25 - loss: 0.169 - ETA: 9:16 - loss: 0.169 - ETA: 9:06 - loss: 0.169 - ETA: 8:56 - loss: 0.169 - ETA: 8:46 - loss: 0.169 - ETA: 8:36 - loss: 0.169 - ETA: 8:26 - loss: 0.169 - ETA: 8:16 - loss: 0.169 - ETA: 8:06 - loss: 0.169 - ETA: 7:57 - loss: 0.169 - ETA: 7:47 - loss: 0.169 - ETA: 7:37 - loss: 0.169 - ETA: 7:27 - loss: 0.169 - ETA: 7:17 - loss: 0.169 - ETA: 7:07 - loss: 0.169 - ETA: 6:58 - loss: 0.169 - ETA: 6:48 - loss: 0.169 - ETA: 6:38 - loss: 0.169 - ETA: 6:28 - loss: 0.169 - ETA: 6:18 - loss: 0.169 - ETA: 6:08 - loss: 0.169 - ETA: 5:59 - loss: 0.169 - ETA: 5:49 - loss: 0.169 - ETA: 5:39 - loss: 0.169 - ETA: 5:29 - loss: 0.169 - ETA: 5:19 - loss: 0.169 - ETA: 5:09 - loss: 0.169 - ETA: 5:00 - loss: 0.169 - ETA: 4:50 - loss: 0.169 - ETA: 4:40 - loss: 0.169 - ETA: 4:30 - loss: 0.169 - ETA: 4:20 - loss: 0.169 - ETA: 4:11 - loss: 0.169 - ETA: 4:01 - loss: 0.169 - ETA: 3:51 - loss: 0.169 - ETA: 3:41 - loss: 0.169 - ETA: 3:31 - loss: 0.169 - ETA: 3:22 - loss: 0.169 - ETA: 3:12 - loss: 0.169 - ETA: 3:02 - loss: 0.169 - ETA: 2:52 - loss: 0.169 - ETA: 2:42 - loss: 0.169 - ETA: 2:33 - loss: 0.168 - ETA: 2:23 - loss: 0.168 - ETA: 2:13 - loss: 0.168 - ETA: 2:03 - loss: 0.168 - ETA: 1:54 - loss: 0.168 - ETA: 1:44 - loss: 0.168 - ETA: 1:34 - loss: 0.168 - ETA: 1:24 - loss: 0.168 - ETA: 1:15 - loss: 0.168 - ETA: 1:05 - loss: 0.168 - ETA: 55s - loss: 0.168 - ETA: 45s - loss: 0.16 - ETA: 35s - loss: 0.16 - ETA: 26s - loss: 0.16 - ETA: 16s - loss: 0.16 - ETA: 6s - loss: 0.1686 - 2172s 9ms/step - loss: 0.1685 - val_loss: 0.1758\n",
      "Epoch 3/7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000/255224 [=======================>......] - ETA: 33:59 - loss: 0.14 - ETA: 33:43 - loss: 0.14 - ETA: 33:37 - loss: 0.13 - ETA: 33:27 - loss: 0.13 - ETA: 33:22 - loss: 0.13 - ETA: 33:17 - loss: 0.13 - ETA: 33:09 - loss: 0.13 - ETA: 32:59 - loss: 0.13 - ETA: 32:47 - loss: 0.13 - ETA: 32:37 - loss: 0.13 - ETA: 32:25 - loss: 0.13 - ETA: 32:15 - loss: 0.13 - ETA: 32:05 - loss: 0.13 - ETA: 31:55 - loss: 0.13 - ETA: 31:44 - loss: 0.13 - ETA: 31:34 - loss: 0.13 - ETA: 31:24 - loss: 0.13 - ETA: 31:13 - loss: 0.13 - ETA: 31:04 - loss: 0.13 - ETA: 30:54 - loss: 0.13 - ETA: 30:44 - loss: 0.13 - ETA: 30:34 - loss: 0.13 - ETA: 30:24 - loss: 0.13 - ETA: 30:14 - loss: 0.13 - ETA: 30:04 - loss: 0.13 - ETA: 29:55 - loss: 0.13 - ETA: 29:46 - loss: 0.13 - ETA: 29:36 - loss: 0.13 - ETA: 29:26 - loss: 0.13 - ETA: 29:16 - loss: 0.13 - ETA: 29:07 - loss: 0.13 - ETA: 28:57 - loss: 0.13 - ETA: 28:48 - loss: 0.13 - ETA: 28:38 - loss: 0.13 - ETA: 28:28 - loss: 0.13 - ETA: 28:18 - loss: 0.13 - ETA: 28:09 - loss: 0.13 - ETA: 27:59 - loss: 0.13 - ETA: 27:50 - loss: 0.13 - ETA: 27:40 - loss: 0.13 - ETA: 27:30 - loss: 0.13 - ETA: 27:20 - loss: 0.13 - ETA: 27:10 - loss: 0.13 - ETA: 27:01 - loss: 0.13 - ETA: 26:51 - loss: 0.13 - ETA: 26:41 - loss: 0.13 - ETA: 26:32 - loss: 0.13 - ETA: 26:22 - loss: 0.13 - ETA: 26:12 - loss: 0.13 - ETA: 26:03 - loss: 0.13 - ETA: 25:53 - loss: 0.13 - ETA: 25:43 - loss: 0.13 - ETA: 25:34 - loss: 0.13 - ETA: 25:24 - loss: 0.13 - ETA: 25:14 - loss: 0.13 - ETA: 25:05 - loss: 0.13 - ETA: 24:55 - loss: 0.13 - ETA: 24:46 - loss: 0.13 - ETA: 24:36 - loss: 0.13 - ETA: 24:26 - loss: 0.13 - ETA: 24:17 - loss: 0.13 - ETA: 24:08 - loss: 0.13 - ETA: 24:00 - loss: 0.13 - ETA: 23:52 - loss: 0.13 - ETA: 23:45 - loss: 0.13 - ETA: 23:40 - loss: 0.13 - ETA: 23:33 - loss: 0.13 - ETA: 23:25 - loss: 0.13 - ETA: 23:17 - loss: 0.13 - ETA: 23:10 - loss: 0.13 - ETA: 23:03 - loss: 0.13 - ETA: 22:54 - loss: 0.13 - ETA: 22:46 - loss: 0.13 - ETA: 22:39 - loss: 0.13 - ETA: 22:32 - loss: 0.13 - ETA: 22:22 - loss: 0.13 - ETA: 22:12 - loss: 0.13 - ETA: 22:03 - loss: 0.13 - ETA: 21:56 - loss: 0.13 - ETA: 21:47 - loss: 0.14 - ETA: 21:39 - loss: 0.14 - ETA: 21:29 - loss: 0.14 - ETA: 21:18 - loss: 0.14 - ETA: 21:08 - loss: 0.14 - ETA: 20:59 - loss: 0.14 - ETA: 20:49 - loss: 0.14 - ETA: 20:40 - loss: 0.14 - ETA: 20:30 - loss: 0.14 - ETA: 20:22 - loss: 0.14 - ETA: 20:12 - loss: 0.14 - ETA: 20:03 - loss: 0.14 - ETA: 19:53 - loss: 0.14 - ETA: 19:43 - loss: 0.14 - ETA: 19:34 - loss: 0.14 - ETA: 19:24 - loss: 0.14 - ETA: 19:16 - loss: 0.14 - ETA: 19:06 - loss: 0.14 - ETA: 18:58 - loss: 0.14 - ETA: 18:49 - loss: 0.14 - ETA: 18:39 - loss: 0.14 - ETA: 18:30 - loss: 0.14 - ETA: 18:20 - loss: 0.14 - ETA: 18:11 - loss: 0.14 - ETA: 18:02 - loss: 0.14 - ETA: 17:52 - loss: 0.14 - ETA: 17:42 - loss: 0.14 - ETA: 17:33 - loss: 0.14 - ETA: 17:23 - loss: 0.14 - ETA: 17:13 - loss: 0.14 - ETA: 17:03 - loss: 0.14 - ETA: 16:53 - loss: 0.14 - ETA: 16:43 - loss: 0.14 - ETA: 16:33 - loss: 0.14 - ETA: 16:23 - loss: 0.14 - ETA: 16:13 - loss: 0.14 - ETA: 16:03 - loss: 0.14 - ETA: 15:54 - loss: 0.14 - ETA: 15:44 - loss: 0.14 - ETA: 15:34 - loss: 0.14 - ETA: 15:24 - loss: 0.14 - ETA: 15:15 - loss: 0.14 - ETA: 15:04 - loss: 0.14 - ETA: 14:54 - loss: 0.14 - ETA: 14:44 - loss: 0.14 - ETA: 14:34 - loss: 0.14 - ETA: 14:24 - loss: 0.14 - ETA: 14:14 - loss: 0.14 - ETA: 14:06 - loss: 0.14 - ETA: 13:56 - loss: 0.14 - ETA: 13:47 - loss: 0.14 - ETA: 13:38 - loss: 0.14 - ETA: 13:28 - loss: 0.14 - ETA: 13:17 - loss: 0.14 - ETA: 13:07 - loss: 0.14 - ETA: 12:57 - loss: 0.14 - ETA: 12:47 - loss: 0.14 - ETA: 12:37 - loss: 0.14 - ETA: 12:27 - loss: 0.14 - ETA: 12:17 - loss: 0.14 - ETA: 12:07 - loss: 0.14 - ETA: 11:57 - loss: 0.14 - ETA: 11:47 - loss: 0.14 - ETA: 11:37 - loss: 0.14 - ETA: 11:27 - loss: 0.14 - ETA: 11:16 - loss: 0.14 - ETA: 11:06 - loss: 0.14 - ETA: 10:56 - loss: 0.14 - ETA: 10:47 - loss: 0.14 - ETA: 10:37 - loss: 0.14 - ETA: 10:27 - loss: 0.14 - ETA: 10:17 - loss: 0.14 - ETA: 10:07 - loss: 0.14 - ETA: 9:56 - loss: 0.1423 - ETA: 9:46 - loss: 0.142 - ETA: 9:36 - loss: 0.142 - ETA: 9:26 - loss: 0.142 - ETA: 9:16 - loss: 0.142 - ETA: 9:06 - loss: 0.142 - ETA: 8:56 - loss: 0.142 - ETA: 8:46 - loss: 0.142 - ETA: 8:36 - loss: 0.142 - ETA: 8:26 - loss: 0.142 - ETA: 8:16 - loss: 0.142 - ETA: 8:07 - loss: 0.142 - ETA: 7:57 - loss: 0.142 - ETA: 7:47 - loss: 0.142 - ETA: 7:37 - loss: 0.142 - ETA: 7:27 - loss: 0.142 - ETA: 7:17 - loss: 0.142 - ETA: 7:06 - loss: 0.142 - ETA: 6:56 - loss: 0.142 - ETA: 6:46 - loss: 0.142 - ETA: 6:36 - loss: 0.142 - ETA: 6:26 - loss: 0.142 - ETA: 6:16 - loss: 0.1428"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=issue_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
